+++
bibkey = "emnlp19data"
bibtex = """@InProceedings{emnlp19data,
  title     = {An Evaluation for Intent Classification and Out-of-Scope Prediction},
  author    = {Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang and Jason Mars},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2019},
  location  = {Hong Kong, China},
  pages     = {},
  url       = {},
  data      = {https://github.com/clinc/oos-eval},
  arxiv     = {https://arxiv.org/abs/1909.02027},
}
"""
title = "An Evaluation for Intent Classification and Out-of-Scope Prediction"
date = "2019-11-01"
draft = false
preprint = false
authors = ["Stefan Larson", "Anish Mahendran", "Joseph J. Peper", "Christopher Clarke", "Andrew Lee", "Parker Hill", "<span style='text-decoration:underline;'>Jonathan K. Kummerfeld</span>", "Kevin Leach", "Michael A. Laurenzano", "Lingjia Tang and Jason Mars"]
publication_types = ["1"]
publication = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
publication_short = "EMNLP (short)"
abstract = "Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scope---i.e., queries that do not fall into any of the system's supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems."
abstract_short = "Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scope---i.e., queries that do not fall into any of the system's supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems."
address = ""
doi = ""
issue = ""
number = ""
pages = ""
publisher = ""
volume = ""
math = true
highlight = false
image_preview = ""
selected = false
url_pdf = "https://arxiv.org/abs/1909.02027"
url_poster = ""
url_interview = ""
url_arxiv = "https://arxiv.org/abs/1909.02027"
url_code = ""
url_dataset = "https://github.com/clinc/oos-eval"
url_project = ""
url_slides = ""
url_video = ""
url_blog = ""



+++