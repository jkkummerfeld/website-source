+++
bibkey = "acl18sql"
bibtex = """@InProceedings{acl18sql,
  author    = {Catherine Finegan-Dollak\\*  and  Jonathan K. Kummerfeld\\*  and  Li Zhang  and  Karthik Ramanathan  and  Sesh Sadasivam  and  Rui Zhang  and  Dragomir Radev},
  title     = {Improving Text-to-SQL Evaluation Methodology},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  shortvenue = {ACL},
  month     = {July},
  year      = {2018},
  address   = {Melbourne, Victoria, Australia},
  pages     = {351--360},
  abstract  = {To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.},
  url       = {http://aclweb.org/anthology/P18-1033},
  software  = {https://jkk.name/text2sql-data},
  data      = {https://jkk.name/text2sql-data},
}
"""
title = "Improving Text-to-SQL Evaluation Methodology"
date = "2018-07-01"
draft = false
preprint = false
authors = ["Catherine Finegan-Dollak\\*", "<span style='text-decoration:underline;'>Jonathan K. Kummerfeld\\*</span>", "Li Zhang", "Karthik Ramanathan", "Sesh Sadasivam", "Rui Zhang", "Dragomir Radev"]
publication_types = ["1"]
publication = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"
publication_short = "ACL"
abstract = "To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development."
abstract_short = "To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development."
address = "Melbourne, Victoria, Australia"
doi = ""
issue = ""
number = ""
pages = "351--360"
publisher = ""
volume = ""
math = true
highlight = false
image_preview = ""
selected = false
url_pdf = "http://aclweb.org/anthology/P18-1033"
url_poster = ""
url_interview = ""
url_code = "https://jkk.name/text2sql-data"
url_dataset = "https://jkk.name/text2sql-data"
url_project = ""
url_slides = ""
url_video = ""

[[citation]]
title = "Dependency-based Hybrid Trees for Semantic Parsing"
year = "2018"
url = "https://arxiv.org/pdf/1809.00107.pdf"
venue = "EMNLP"
author = "Zhanming Jie, Wei Lu"


+++