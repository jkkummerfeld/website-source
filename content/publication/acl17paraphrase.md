+++
bibkey = "acl17paraphrase"
bibtex = """@InProceedings{acl17paraphrase,
  author    = {Youxuan Jiang and Jonathan K. Kummerfeld and Walter S. Lasecki},
  title     = {Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  location  = {Vancouver, Canada},
  pages     = {103--109},
  url       = {http://www.aclweb.org/anthology/P17-2017.pdf},
  data      = {http://aclweb.org/anthology/attachments/P/P17/P17-2017.Datasets.zip},
  slidespdf = {https://www.aclweb.org/anthology/attachments/P17-2017.Presentation.pdf},
  video     = {https://vimeo.com/234958413},
  arxiv     = {https://arxiv.org/abs/1704.05753},
}
"""
title = "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection"
date = "2017-07-01"
draft = false
preprint = false
authors = ["Youxuan Jiang", "<span style='text-decoration:underline;'>Jonathan K. Kummerfeld</span>", "Walter S. Lasecki"]
publication_types = ["1"]
publication = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"
publication_short = "ACL (short)"
abstract = "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures."
abstract_short = "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures."
address = ""
doi = ""
issue = ""
number = ""
pages = "103--109"
publisher = ""
volume = ""
math = true
highlight = false
image_preview = ""
selected = false
url_pdf = "http://www.aclweb.org/anthology/P17-2017.pdf"
url_poster = ""
url_interview = ""
url_arxiv = "https://arxiv.org/abs/1704.05753"
url_code = ""
url_dataset = "http://aclweb.org/anthology/attachments/P/P17/P17-2017.Datasets.zip"
url_project = ""
url_slides = ""
url_video = "https://vimeo.com/234958413"
url_blog = ""
url_slides_pdf = "https://www.aclweb.org/anthology/attachments/P17-2017.Presentation.pdf"

[[citation]]
title = "Effective Crowdsourced Generation of Training Data for Chatbots Natural Language Understanding"
year = "2018"
url = "https://link.springer.com/chapter/10.1007/978-3-319-91662-0_8"
venue = "ICWE"
author = "R. Bapat, P. Kucherbaev, and A. Bozzon"

[[citation]]
title = "SPADE: Evaluation Dataset for Monolingual Phrase Alignment"
year = "2018"
url = "http://www.lrec-conf.org/proceedings/lrec2018/pdf/502.pdf"
venue = "LREC"
author = "Yuki Arase1, Junichi Tsujii?"

[[citation]]
title = "Crowdsourcing for Reminiscence Chatbot Design"
year = "2018"
url = "https://arxiv.org/pdf/1805.12346.pdf"
venue = "HCOMP"
author = "Svetlana Nikitina, Florian Daniel, Marcos Baez, Fabio Casati"

[[citation]]
title = "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users"
year = "2018"
url = "http://dx.doi.org/10.1145/3234695.3236343"
venue = "ASSETS"
author = "Raymond Fok, Harmanpreet Kaur, Skanda Palani, Martez E. Mott and Walter S. Lasecki"

[[citation]]
title = "A Study of Incorrect Paraphrases in Crowdsourced User Utterances"
year = "2019"
url = "https://www.aclweb.org/anthology/N19-1026.pdf"
venue = "NAACL"
author = "Mohammad-Ali Yaghoub-Zadeh-Fard, Boualem Benatallah, Moshe Chai Barukh, Shayan Zamanirad"

[[citation]]
title = "Optimizing the Design and Cost for Crowdsourced Conversational Utterances"
year = "2019"
url = ""
venue = "Workshop on Data Collection, Curation, and Labeling (DCCL) for Mining and Learning"
author = "Phoebe Liu, Joan Xiao, Tong Liu, Dylan F. Glas"

[[citation]]
title = "Personalizing crowdsourced human-robot interaction through curiosity-driven learning"
year = "2019"
url = ""
venue = "Personalization in Long-Term Human-Robot Interaction"
author = "Phoebe Liu, Malcolm Doering, Dylan F. Glas, Takayuki Kanda, Dana Kulic, Hiroshi Ishiguro"

[[citation]]
title = "PKU Paraphrase Bank: A Sentence-Level Paraphrase Corpus for Chinese"
year = "2019"
url = ""
venue = "Natural Language Processing and Chinese Computing"
author = "Bowei Zhang, Weiwei Sun, Xiaojun Wan, Zongming Guo"

[[citation]]
title = "Efficient Elicitation Approaches to Estimate Collective Crowd Answers"
year = "2019"
url = ""
venue = "CSCW"
author = "John Joon Young Chung, Jean Y. Song, Sindhu Kutty, Sungsoo (Ray) Hong, Juho Kim, Walter S. Lasecki"

[[citation]]
title = "Optimizing for Happiness and Productivity: Modeling Opportune Moments for Transitions and Breaks at Work"
year = "2020"
url = ""
venue = "CHI"
author = "Harmanpreet Kaur, Alex C. Williams, Daniel McDuff, Mary Czerwinski, Jaime Teevan, Shamsi Iqbal"


+++