---

bibkey: coling20personal

title: Exploring the Value of Personalized Word Embeddings

date: "2020-12-01"

year: 2020

draft: false

preprint: false

archival: true

authors: 
- Charles Welch
- admin
- Verónica Pérez-Rosas
- Rada Mihalcea

publication_types: ["1"]

publication: Proceedings of the 28th International Conference on Computational Linguistics

publication_short: CoLing (short)

abstract: In this paper, we introduce personalized word embeddings, and examine their value for language modeling. We compare the performance of our proposed prediction model when using personalized versus generic word representations, and study how these representations can be leveraged for improved performance. We provide insight into what types of words can be more accurately predicted when building personalized models. Our results show that a subset of words belonging to specific psycholinguistic categories tend to vary more in their representations across users and that combining generic and personalized word embeddings yields the best performance, with a 4.7{%} relative reduction in perplexity. Additionally, we show that a language model using personalized word embeddings can be effectively used for authorship attribution.

abstract_short: In this paper, we introduce personalized word embeddings, and examine their value for language modeling. We compare the performance of our proposed prediction model when using personalized versus generic word representations, and study how these representations can be leveraged for improved performance. We provide insight into what types of words can be more accurately predicted when building personalized models. Our results show that a subset of words belonging to specific psycholinguistic categories tend to vary more in their representations across users and that combining generic and personalized word embeddings yields the best performance, with a 4.7{%} relative reduction in perplexity. Additionally, we show that a language model using personalized word embeddings can be effectively used for authorship attribution.

address: 

doi: 

issue: 

number: 

pages: 6856--6862

publisher: 

volume: 

math: true

highlight: false

image_preview: 

selected: false

url_pdf: "https://www.aclweb.org/anthology/2020.coling-main.604"

url_poster: 

url_interview: 

url_arxiv: 

url_code: 

url_dataset: 

url_project: 

url_slides: 

url_video: 

url_blog: 

links: 

citation_count: 0
citations:


---