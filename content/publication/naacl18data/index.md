---

bibkey: naacl18data

title: "Data Collection for a Production Dialogue System: A Startup Perspective"

date: "2018-06-01"

year: 2018

draft: false

preprint: false

archival: true

authors: 
- Yiping Kang
- Yunqi Zhang
- admin
- Parker Hill
- Johann Hauswald
- Michael A. Laurenzano
- Lingjia Tang
- Jason Mars

publication_types: ["1"]

publication: "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)"

publication_short: NAACL (industry)

abstract: Industrial dialogue systems such as Apple Siri and Google Assistant require large scale diverse training data to enable their sophisticated conversation capabilities. Crowdsourcing is a scalable and inexpensive data collection method, but collecting high quality data efficiently requires thoughtful orchestration of crowdsourcing jobs. Prior study of data collection process has focused on tasks with limited scope and performed intrinsic data analysis, which may not be indicative of impact on trained model performance. In this paper, we present a study of crowdsourcing methods for a user intent classification task in one of our deployed dialogue systems. Our task requires classification over 47 possible user intents and contains many intent pairs with subtle differences. We consider different crowdsourcing job types and job prompts, quantitatively analyzing the quality of collected data and downstream model performance on a test set of real user queries from production logs. Our observations provide insight into how design decisions impact crowdsourced data quality, with clear recommendations for future data collection for dialogue systems.

abstract_short: Industrial dialogue systems such as Apple Siri and Google Assistant require large scale diverse training data to enable their sophisticated conversation capabilities. Crowdsourcing is a scalable and inexpensive data collection method, but collecting high quality data efficiently requires thoughtful orchestration of crowdsourcing jobs. Prior study of data collection process has focused on tasks with limited scope and performed intrinsic data analysis, which may not be indicative of impact on trained model performance. In this paper, we present a study of crowdsourcing methods for a user intent classification task in one of our deployed dialogue systems. Our task requires classification over 47 possible user intents and contains many intent pairs with subtle differences. We consider different crowdsourcing job types and job prompts, quantitatively analyzing the quality of collected data and downstream model performance on a test set of real user queries from production logs. Our observations provide insight into how design decisions impact crowdsourced data quality, with clear recommendations for future data collection for dialogue systems.

address: 

doi: 10.18653/v1/N18-3005

issue: 

number: 

pages: 33--40

publisher: 

volume: 

math: true

highlight: false

image_preview: 

selected: false

url_pdf: "https://aclanthology.org/N18-3005.pdf"

url_poster: 

url_interview: 

url_arxiv: 

url_code: 

url_dataset: 

url_project: 

url_slides: 

url_video: "https://vimeo.com/277631102"

url_blog: 

links: 

citation_count: 12
citations:
- title: Enhancing Domain-Specific Supervised Natural Language Intent Classification with a Top-Down Selective Ensemble Model
  year: 2019
  url: "http://www.mdpi.com/2504-4990/1/2/37"
  venue: Machine Learning and Knowledge Extraction
  authors: Gard B. Jenset, Barbara McGillivray
- title: A Study of Incorrect Paraphrases in Crowdsourced User Utterances
  year: 2019
  url: "https://www.aclweb.org/anthology/N19-1026.pdf"
  venue: NAACL
  authors: Mohammad-Ali Yaghoub-Zadeh-Fard, Boualem Benatallah, Moshe Chai Barukh, Shayan Zamanirad
- title: Data Collection Methods for Building a Free Response Training Simulation
  year: 2019
  url: 
  venue: Systems and Information Engineering Design Symposium
  authors: Vaibhav Sharma, Beni Shpringer, Sung Min Yang, Martin Bolger, Sodiq Adewole, Dr. D. Brown, Erfaneh Gharavi
- title: Personalizing crowdsourced human-robot interaction through curiosity-driven learning
  year: 2019
  url: 
  venue: Personalization in Long-Term Human-Robot Interaction
  authors: Phoebe Liu, Malcolm Doering, Dylan F. Glas, Takayuki Kanda, Dana Kulic, Hiroshi Ishiguro
- title: "MA-DST: Multi-Attention-Based Scalable Dialog State Tracking"
  year: 2019
  url: 
  venue: "The 3rd NeurIPS workshop on Conversational AI: Today's Practice and Tomorrow's Potential"
  authors: Adarsh Kumar, Peter Ku, Anuj Kumar Goyal, Angeliki Metallinou, Dilek Hakkani-tur
- title: "User Utterance Acquisition for Training Task-Oriented Bots: A Review of Challenges, Techniques and Opportunities"
  year: 2020
  url: 
  venue: IEEE Internet Computing
  authors: Mohammad-Ali Yaghoub-Zadeh-Fard, Boualem Benatallah, Fabio Casati, Moshe Chai Barukh, Shayan Zamanirad
- title: Dynamic word recommendation to obtain diverse crowdsourced paraphrases of user utterances
  year: 2020
  url: 
  venue: IUI
  authors: Mohammad-Ali Yaghoub-Zadeh-Fard, Boualem Benatallah, Fabio Casati, Moshe Chai Barukh, Shayan Zamanirad
- title: Data Query Language and Corpus Tools for Slot-Filling and Intent Classification Data
  year: 2020
  url: 
  venue: LREC
  authors: Stefan Larson, Eric Guldan, Kevin Leach
- title: More Diverse Dialogue Datasets via Diversity-Informed Data Collection
  year: 2020
  url: 
  venue: ACL
  authors: Katherine Stasaski, Grace Hui Yang, Marti A. Hearst
- title: Optimizing the Design and Cost for Crowdsourced Conversational Utterances
  year: 2019
  url: 
  venue: "KDD WOrkshop: Data Collection, Curation, and Labeling (DCCL) for Mining and Learning"
  authors: Phoebe Liu, Joan Xiao, Tong Liu, Dylan F. Glas
- title: Dialogue Act Classification for Virtual Agents for Software Engineers during Debugging
  year: 2020
  url: 
  venue: International Conference on Software Engineering Workshops
  authors: Andrew Wood, Zachary Eberhart, Collin McMillan
- title: A Wizard of Oz Study Simulating API Usage Dialogues with a Virtual Assistant
  year: 2020
  url: 
  venue: IEEE Transactions on Software Engineering
  authors: Zachary Eberhart, Aakash Bansal, Collin McMillan


---