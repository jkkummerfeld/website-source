+++
bibkey = "naacl18data"
bibtex = """@InProceedings{naacl18data,
  author    = {Yiping Kang and Yunqi Zhang and Jonathan K. Kummerfeld and Parker Hill and Johann Hauswald and Michael A. Laurenzano and Lingjia Tang and Jason Mars},
  title     = {Data Collection for a Production Dialogue System: A Startup Perspective},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)},
  pages     = {33--40},
  year      = {2018},
  month     = {June},
  location  = {New Orleans, Louisiana, USA},
  url       = {http://www.aclweb.org/anthology/N18-3005.pdf},
  video     = {https://vimeo.com/277631102},
}
"""
title = "Data Collection for a Production Dialogue System: A Startup Perspective"
date = "2018-06-01"
draft = false
preprint = false
archival = true
authors = ["Yiping Kang", "Yunqi Zhang", "<span style='text-decoration:underline;'>Jonathan K. Kummerfeld</span>", "Parker Hill", "Johann Hauswald", "Michael A. Laurenzano", "Lingjia Tang", "Jason Mars"]
publication_types = ["1"]
publication = "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)"
publication_short = "NAACL (industry)"
abstract = "Industrial dialogue systems such as Apple Siri and Google Assistant require large scale diverse training data to enable their sophisticated conversation capabilities. Crowdsourcing is a scalable and inexpensive data collection method, but collecting high quality data efficiently requires thoughtful orchestration of crowdsourcing jobs. Prior study of data collection process has focused on tasks with limited scope and performed intrinsic data analysis, which may not be indicative of impact on trained model performance. In this paper, we present a study of crowdsourcing methods for a user intent classification task in one of our deployed dialogue systems. Our task requires classification over 47 possible user intents and contains many intent pairs with subtle differences. We consider different crowdsourcing job types and job prompts, quantitatively analyzing the quality of collected data and downstream model performance on a test set of real user queries from production logs. Our observations provide insight into how design decisions impact crowdsourced data quality, with clear recommendations for future data collection for dialogue systems."
abstract_short = "Industrial dialogue systems such as Apple Siri and Google Assistant require large scale diverse training data to enable their sophisticated conversation capabilities. Crowdsourcing is a scalable and inexpensive data collection method, but collecting high quality data efficiently requires thoughtful orchestration of crowdsourcing jobs. Prior study of data collection process has focused on tasks with limited scope and performed intrinsic data analysis, which may not be indicative of impact on trained model performance. In this paper, we present a study of crowdsourcing methods for a user intent classification task in one of our deployed dialogue systems. Our task requires classification over 47 possible user intents and contains many intent pairs with subtle differences. We consider different crowdsourcing job types and job prompts, quantitatively analyzing the quality of collected data and downstream model performance on a test set of real user queries from production logs. Our observations provide insight into how design decisions impact crowdsourced data quality, with clear recommendations for future data collection for dialogue systems."
address = ""
doi = ""
issue = ""
number = ""
pages = "33--40"
publisher = ""
volume = ""
math = true
highlight = false
image_preview = ""
selected = false
url_pdf = "http://www.aclweb.org/anthology/N18-3005.pdf"
url_poster = ""
url_interview = ""
url_arxiv = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = "https://vimeo.com/277631102"
url_blog = ""

[[citation]]
title = "Enhancing Domain-Specific Supervised Natural Language Intent Classification with a Top-Down Selective Ensemble Model"
year = "2019"
url = "http://www.mdpi.com/2504-4990/1/2/37"
venue = "Machine Learning and Knowledge Extraction"
author = "Gard B. Jenset, Barbara McGillivray"

[[citation]]
title = "A Study of Incorrect Paraphrases in Crowdsourced User Utterances"
year = "2019"
url = "https://www.aclweb.org/anthology/N19-1026.pdf"
venue = "NAACL"
author = "Mohammad-Ali Yaghoub-Zadeh-Fard, Boualem Benatallah, Moshe Chai Barukh, Shayan Zamanirad"

[[citation]]
title = "Data Collection Methods for Building a Free Response Training Simulation"
year = "2019"
url = ""
venue = "Systems and Information Engineering Design Symposium"
author = "Vaibhav Sharma, Beni Shpringer, Sung Min Yang, Martin Bolger, Sodiq Adewole, Dr. D. Brown, Erfaneh Gharavi"

[[citation]]
title = "Personalizing crowdsourced human-robot interaction through curiosity-driven learning"
year = "2019"
url = ""
venue = "Personalization in Long-Term Human-Robot Interaction"
author = "Phoebe Liu, Malcolm Doering, Dylan F. Glas, Takayuki Kanda, Dana Kulic, Hiroshi Ishiguro"

[[citation]]
title = "MA-DST: Multi-Attention-Based Scalable Dialog State Tracking"
year = "2019"
url = ""
venue = "The 3rd NeurIPS workshop on Conversational AI: Today's Practice and Tomorrow's Potential"
author = "Adarsh Kumar, Peter Ku, Anuj Kumar Goyal, Angeliki Metallinou, Dilek Hakkani-tur"


+++